# -*- coding: utf-8 -*-
"""RL_DP_part1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E0cjLutFYmEbNW2a2vqKhTO56WaATINg

# Gym Environments in Colab
"""

import numpy as np
import matplotlib.pyplot as plt
import gym
from gym import spaces

#!pip -q install pyglet==1.4.0

"""# Grid Environment -- Part 1 Deterministic"""

tot_states = 16
grid_size = 4
actions = 4
tot_rewards = 5
neg_rewards = 2
pos_rewards = tot_rewards - neg_rewards
gamma = 0.9

import random
#uncomment the following to check positions and the reward placed in them.   In case of Dynamic environment building

#first_index = np.random.random_integers(0,grid_size-1, size=(1,tot_rewards)).tolist()[0] 
#sec_index = np.random.random_integers(0,grid_size-1, size=(1,tot_rewards)).tolist()[0]
#print("Reward Distribution")
#for i in range(tot_rewards):
#  if(i<neg_rewards):
#    print('( %d , %d ) --> +3' %(first_index[i] ,sec_index[i]) )
#  else:
#    print('( %d , %d ) --> +2' %(first_index[i] ,sec_index[i]) )

class GridEnvironment(gym.Env):
    metadata = { 'render.modes': [] }
    
    def __init__(self):
        self.observation_space = spaces.Discrete(tot_states)
        self.action_space = spaces.Discrete(grid_size)
        self.max_timesteps = 10 
        self.total_reward = 0

    def reset(self):
        self.timestep = 0
        self.agent_pos = [0, 0]
        self.goal_pos = [grid_size-1, grid_size-1]
        self.state = np.zeros((grid_size,grid_size)) 

        #Uncomment the following lines to generate dynamic environment
        # for i in range(tot_rewards):
        #   if(i<neg_rewards):
        #     self.state[(first_index[i],sec_index[i])] = 3
        #   else:
        #     self.state[(first_index[i],sec_index[i])] = 2

        #Comment the following lines to generate dynamic environment
        self.state[(2,3)] = 3
        self.state[(0,3)] = 3
        self.state[(2,1)] = 2
        self.state[(1,3)] = 2
        self.state[(2,2)] = 2 
        
        self.state[tuple(self.agent_pos)] = -1
        self.state[tuple(self.goal_pos)] = 5
        observation = self.state.flatten() 
        return observation
    
    def step(self, action):
        # self.state = np.random.choice(self.observation_space.n)
        prev_pos = self.agent_pos
        self.state[tuple(self.goal_pos)] = 5
        #self.state[tuple([0,0])] = 0

        if action == 0:         # down
          self.agent_pos[0] += 1
        if action == 1:         # up
          self.agent_pos[0] -= 1
        if action == 2:         # right
          self.agent_pos[1] += 1
        if action == 3:         # left 
          self.agent_pos[1] -= 1

        self.agent_pos = np.clip(self.agent_pos, 0, grid_size-1)

        self.total_reward = 0
        # if(self.agent_pos ==  prev_pos).all():
        #   self.tot_rewards = 0
        # else:
        self.total_reward =  self.state[tuple(self.agent_pos)] 
          
        self.state = np.zeros((grid_size,grid_size))

        #Uncomment the following lines to generate dynamic environment
        # for i in range(tot_rewards):
        #   if(i<neg_rewards):
        #     self.state[(first_index[i],sec_index[i])] = 3
        #   else:
        #     self.state[(first_index[i],sec_index[i])] = 2

        #Comment the following lines to generate dynamic environment
        self.state[(2,3)] = 3
        self.state[(0,3)] = 3
        self.state[(2,1)] = 2
        self.state[(1,3)] = 2
        self.state[(2,2)] = 2 

        self.state[tuple(self.agent_pos)] = -1
        if(self.agent_pos ==  self.goal_pos).all():
          self.state[tuple(self.goal_pos)] = -1
        else:
          self.state[tuple(self.goal_pos)] = 5
        observation = self.state.flatten()
        #print(observation)
        self.timestep += 1
        done = True if (self.timestep >= self.max_timesteps or (self.agent_pos == self.goal_pos).all()) else False
        info = {}
        
        return observation, self.total_reward, done, info
        
    def render(self):
        plt.imshow(self.state)

env = GridEnvironment()
obs = env.reset()
env.render()

"""## A Random Agent with Dynamic Programming as Tabular method"""

class RandomAgent: 
    def __init__(self, env):
        self.env = env
        self.observation_space = env.observation_space
        self.action_space = env.action_space  

    def step(self, observation): 
        agent_pos = np.where(observation == -1)
        temp = self.Policy(observation)
        #print(temp)
        transitions = self.trans() 
        s_trans = transitions[agent_pos[0][0]]
        x = 0
        va = [0]*len(s_trans)
        for k in s_trans:
          if((agent_pos[0][0]) == int(k)):
            va[x] = -9999
          else:
            va[x] = temp[int(k)]
          x = x + 1
        max_ele = np.amax(va)
        indexes = np.where(va == max_ele) 
        actionlist = indexes[0] 
        return np.random.choice(actionlist)
 
    def Policy(self,observation):
        policy = np.zeros((tot_states,actions)) 
        temp_0 = [0] * tot_states
        temp_1 = [0] * tot_states
        transitions = self.trans()
        theta = 0.02
        delta = 10
        while (delta > theta): #for k in range(5) 
          delta = 0
          for i in range(tot_states): 
            v = 0  
            s_trans = transitions[i]
            x = 0
            va = [0]*len(s_trans)
            for k in s_trans:
              va[x] = temp_0[int(k)]
              x = x + 1
            max_ele = np.amax(va)
            indexes = np.where(va == max_ele)
            t = 0 
            for p in range((actions)):#indexes[0]:
              a = int(i / grid_size)
              b = int(i % grid_size)
              env = GridEnvironment()
              env.reset()
              policy[i][p] = policy[i][p] + 1
              env.agent_pos[0] = a
              env.agent_pos[1] = b
              obs, reward, done, info = env.step(p) 
              s_dash = int(transitions[i][p])   
              #pos_actions = np.where(transitions[i] != i)[0]
              #no = len(pos_actions) 
              #if(int(s_dash) == int(i)):
              #  t = t 
              #else: 
              #  val_s_dash =   temp_0[s_dash] 
              t = t + (1/(actions))*(reward + gamma * temp_0[s_dash]) #len(indexes[0])
            delta = max(delta,abs(temp_0[i] - t))
            temp_1[i] = t 

          for i in range(len(temp_0)):
            temp_0[i] = temp_1[i]  
        return temp_0

    def trans(self):
        env = GridEnvironment()
        env.reset()
        trans = np.zeros((tot_states,actions))
        for i in range(tot_states):
          trans[i] = [i] * actions 
        for i in range(tot_states):
          a = int(i / grid_size)
          b = int(i % grid_size)
          for j in range(actions):
            env.agent_pos[0] = a 
            env.agent_pos[1] = b
            obs, reward, done, info = env.step(j) 
            agent_position = np.where(obs == -1)  
            trans[i][j] = agent_position[0] 
        return trans

"""## Running a Markov Decision Process"""

# setting up environment and agent
env = GridEnvironment()
agent = RandomAgent(env)
obs = env.reset()
done = False
# MDP Loop
tot_reward = 0
print("Deterministic Environment Started")
while not done:
   env.render()
   plt.show()
   action = agent.step(obs)
   print("action to be performed: %d" %(action))
   obs, reward, done, info = env.step(action)
   tot_reward = tot_reward + reward
   print("cummulative total reward: %d" %(tot_reward))
env.render()

"""#Stochastic Environment"""

class GridEnvironment_sto(gym.Env):
    metadata = { 'render.modes': [] }
    
    def __init__(self):
        self.observation_space = spaces.Discrete(tot_states)
        self.action_space = spaces.Discrete(grid_size)
        self.max_timesteps = 10 
        self.total_reward = 0  

    def reset(self):
        self.timestep = 0
        self.agent_pos = [0, 0]
        self.goal_pos = [grid_size-1, grid_size-1]
        self.state = np.zeros((grid_size,grid_size)) 

        # for i in range(tot_rewards):
        #   if(i<neg_rewards):
        #     self.state[(first_index[i],sec_index[i])] = 3
        #   else:
        #     self.state[(first_index[i],sec_index[i])] = 2

        self.state[(2,3)] = 3
        self.state[(0,3)] = 3
        self.state[(2,1)] = 2
        self.state[(1,3)] = 2
        self.state[(2,2)] = 2
        
        self.state[tuple(self.agent_pos)] = -1
        self.state[tuple(self.goal_pos)] = 5
        observation = self.state.flatten() 
        return observation
    
    def step(self, action):
        # self.state = np.random.choice(self.observation_space.n)
        prev_pos = self.agent_pos
        self.state[tuple(self.goal_pos)] = 5
        #self.state[tuple([0,0])] = 0  #np.where(observation == -1)[0][0]
        rand = random.randint(1,100) / 100
        prob = 0.8
        if action == 0 and rand < prob :         # down
          self.agent_pos[0] += 1
        if action == 1 and rand < prob :         # up
          self.agent_pos[0] -= 1
        if action == 2 and rand < prob :         # right
          self.agent_pos[1] += 1
        if action == 3 and rand < prob :         # left 
          self.agent_pos[1] -= 1

        self.agent_pos = np.clip(self.agent_pos, 0, grid_size-1)

        self.total_reward = 0

        #if(self.agent_pos ==  prev_pos).all():
        #   self.tot_rewards = 0
        #else:
        self.total_reward = self.state[tuple(self.agent_pos)] 
        print("current state reward: %d" %(self.total_reward))
          
        self.state = np.zeros((grid_size,grid_size))

        # for i in range(tot_rewards):
        #   if(i<neg_rewards):
        #     self.state[(first_index[i],sec_index[i])] = 3
        #   else:
        #     self.state[(first_index[i],sec_index[i])] = 2

        self.state[(2,3)] = 3
        self.state[(0,3)] = 3
        self.state[(2,1)] = 2
        self.state[(1,3)] = 2
        self.state[(2,2)] = 2
        
        self.state[tuple(self.agent_pos)] = -1
        if(self.agent_pos ==  self.goal_pos).all():
          self.state[tuple(self.goal_pos)] = -1
        else:
          self.state[tuple(self.goal_pos)] = 5
        observation = self.state.flatten()
        #print(observation)
        
        self.timestep += 1
        done = True if (self.timestep >= self.max_timesteps or (self.agent_pos == self.goal_pos).all()) else False
        info = {}
        
        return observation, self.total_reward, done, info
        
    def render(self):
        plt.imshow(self.state)

"""#Random Agent with Dynamic Programming as Tabular method"""

class RandomAgent: 
    def __init__(self, env):
        self.env = env
        self.observation_space = env.observation_space
        self.action_space = env.action_space   

    def step(self, observation): 
        agent_pos = np.where(observation == -1)
        transitions =  self.trans() 
        #print(transitions)
        temp = self.Policy(observation,transitions)
        #print(temp)
        s_trans = transitions[agent_pos[0][0]]
        x = 0
        va = [0]*len(s_trans)
        for k in s_trans:
          if((agent_pos[0][0]) == int(k)):
            va[x] = -9999
          else:
            va[x] = temp[int(k)]
          x = x + 1
        max_ele = np.amax(va)
        indexes = np.where(va == max_ele) 
        actionlist = indexes[0] 
        return np.random.choice(actionlist)

    def probability_matrix(self,trans):
        t = trans  
        prob = np.zeros((tot_states,actions))
        for i in range(tot_states):
          pos_states = t[i]
          for j in range(len(pos_states)):
            if(pos_states[j] == i):
              prob[i][j] = 0
            else:
              prob[i][j] = 0.8
        return prob

    def Policy(self,observation,trans):
        policy = np.zeros((tot_states,actions)) 
        temp_0 = [0] * tot_states
        temp_1 = [0] * tot_states
        transitions = trans
        probs = self.probability_matrix(transitions)
        #print(probs)
        theta = 0.02
        delta = 10
        while (delta > theta): #for k in range(5) 
          delta = 0
          for i in range(tot_states): 
            v = 0  
            s_trans = transitions[i]
            x = 0
            va = [0]*len(s_trans)
            for k in s_trans:
              va[x] = temp_0[int(k)]
              x = x + 1
            max_ele = np.amax(va)
            indexes = np.where(va == max_ele)
            t = 0 
            for p in range((actions)):#indexes[0]:
              state_prob = probs[i][p]
              a = int(i / grid_size)
              b = int(i % grid_size)
              env = GridEnvironment()
              env.reset()
              policy[i][p] = policy[i][p] + 1
              env.agent_pos[0] = a
              env.agent_pos[1] = b
              obs, reward, done, info = env.step(p) 
              s_dash = int(transitions[i][p])   
              #pos_actions = np.where(transitions[i] != i)[0]
              #no = len(pos_actions) 
              #if(int(s_dash) == int(i)):
              #  t = t 
              #else: 
              #  val_s_dash =   temp_0[s_dash] 
              t = t + (state_prob)*(1/(actions))*(reward + gamma * temp_0[s_dash]) + (1-state_prob)*(1/(actions))*(reward + gamma * temp_0[i])  #len(indexes[0])
            delta = max(delta,abs(temp_0[i] - t))
            temp_1[i] = t 

          for i in range(len(temp_0)):
            temp_0[i] = temp_1[i]  
        return temp_0

    def trans(self):
        env = GridEnvironment()
        env.reset()
        trans = np.zeros((tot_states,actions))
        for i in range(tot_states):
          trans[i] = [i] * actions 
        for i in range(tot_states):
          a = int(i / grid_size)
          b = int(i % grid_size)
          for j in range(actions):
            env.agent_pos[0] = a 
            env.agent_pos[1] = b
            obs, reward, done, info = env.step(j) 
            agent_position = np.where(obs == -1)  
            trans[i][j] = agent_position[0] 
        return trans

# setting up environment and agent
env = GridEnvironment_sto()
agent = RandomAgent(env)
obs = env.reset()
done = False
# MDP Loop
tot_reward = 0
print("Stochastic Environment Started")
while not done:
   env.render()
   plt.show()
   action = agent.step(obs)
   print("action to be performed %d" %(action))
   obs, reward, done, info = env.step(action)
   tot_reward = tot_reward + reward
   print("cummulative total reward: %d" %(tot_reward))
env.render()

